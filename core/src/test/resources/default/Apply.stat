80 columns                                                                     |
<<< Massive 1
List(Split(Space, 0).withPolicy(SingleLineBlock(close)),
           Split(nl, 1).withPolicy({
             case Decision(t@FormatToken(_, `close`, _), s) =>
               Decision(t, List(Split(Newline, 0)))
           }).withIndent(2, close, Right))
>>>
List(Split(Space, 0).withPolicy(SingleLineBlock(close)),
     Split(nl, 1)
       .withPolicy({
         case Decision(t@FormatToken(_, `close`, _), s) =>
           Decision(t, List(Split(Newline, 0)))
       })
       .withIndent(2, close, Right))
<<< Massive 2
 List(Split(Space,
            0,
            policy = SingleLineBlock(close),
            ignoreIf = blockSize > style.maxColumn),
      Split(nl,
            1,
            policy = {
            case Decision(t@FormatToken(_, `close`, _), s) =>
            Decision(t, List(Split(Newline, 0)))
            }))
>>>
List(Split(Space,
           0,
           policy = SingleLineBlock(close),
           ignoreIf = blockSize > style.maxColumn),
     Split(nl, 1, policy = {
       case Decision(t@FormatToken(_, `close`, _), s) =>
         Decision(t, List(Split(Newline, 0)))
     }))
<<< Massive (good API) 3
Split(Space, 0).withPolicy {
    // Following case:
    // package foo // this is cool
    //
    // object a
    case Decision(t@FormatToken(`lastRef`, _: Comment, between), splits)
        if !between.exists(_.isInstanceOf[`\n`]) =>
      Decision(t, splits.map(_.withModification(Space)))
    case Decision(t@FormatToken(`lastRef`, _, _), splits) =>
      Decision(t, splits.map(_.withModification(Newline2x)))
}
>>>
Split(Space, 0).withPolicy {
  // Following case:
  // package foo // this is cool
  //
  // object a
  case Decision(t@FormatToken(`lastRef`, _: Comment, between), splits)
      if !between.exists(_.isInstanceOf[`\n`]) =>
    Decision(t, splits.map(_.withModification(Space)))
  case Decision(t@FormatToken(`lastRef`, _, _), splits) =>
    Decision(t, splits.map(_.withModification(Newline2x)))
}
<<< Massive (bad API) 3
Split(Space, 0, policy = {
    // Following case:
    // package foo // this is cool
    //
    // object a
    case Decision(t@FormatToken(`lastRef`, _: Comment, between), splits)
        if !between.exists(_.isInstanceOf[`\n`]) =>
      Decision(t, splits.map(_.withModification(Space)))
    case Decision(t@FormatToken(`lastRef`, _, _), splits) =>
      Decision(t, splits.map(_.withModification(Newline2x)))
})
>>>
Split(Space, 0, policy = {
  // Following case:
  // package foo // this is cool
  //
  // object a
  case Decision(t@FormatToken(`lastRef`, _: Comment, between), splits)
      if !between.exists(_.isInstanceOf[`\n`]) =>
    Decision(t, splits.map(_.withModification(Space)))
  case Decision(t@FormatToken(`lastRef`, _, _), splits) =>
    Decision(t, splits.map(_.withModification(Newline2x)))
})
<<< Looong
Seq(
  Split(modification, 0, policy = singleLine)
    .withIndent(indent, close, Left)
     .withOptimal(optimalTok),
  Split(Newline, 1 + nestedPenalty + lhsPenalty, policy = singleLine)
    .withIndent(indent, close, Left)
    .withOptimal(optimalTok),
  Split(modification, 2 + lhsPenalty, policy = oneArgOneLine, ignoreIf = singleArgument)
    .withIndent(StateColumn, close, Right)
    .withOptimal(optimalTok),
  Split(Newline,
    3 + nestedPenalty + lhsPenalty,
    policy = oneArgOneLine, ignoreIf = singleArgument)
    .withIndent(indent, close, Left)

)
>>>
Seq(
    Split(modification, 0, policy = singleLine)
      .withIndent(indent, close, Left)
      .withOptimal(optimalTok),
    Split(Newline, 1 + nestedPenalty + lhsPenalty, policy = singleLine)
      .withIndent(indent, close, Left)
      .withOptimal(optimalTok),
    Split(modification,
          2 + lhsPenalty,
          policy = oneArgOneLine,
          ignoreIf = singleArgument)
      .withIndent(StateColumn, close, Right)
      .withOptimal(optimalTok),
    Split(Newline,
          3 + nestedPenalty + lhsPenalty,
          policy = oneArgOneLine,
          ignoreIf = singleArgument).withIndent(indent, close, Left)
)
<<< continuation indent
Seq(
            // Either everything fits in one line or break on =>
            Split(Space, 0).withPolicy(SingleLineBlock(lastToken)),
            Split(Space, 1).withPolicy(Policy({
                  case Decision(t@FormatToken(`arrow`, _, _), s) =>
                    Decision(t, s.filter(_.modification.isNewline))
                },
                expire = lastToken.end))
              .withIndent(2, lastToken, Left) // case body indented by 2.
              .withIndent(2, arrow, Left) // cond body indented by 4.
        )
>>>
Seq(
    // Either everything fits in one line or break on =>
    Split(Space, 0).withPolicy(SingleLineBlock(lastToken)),
    Split(Space, 1)
      .withPolicy(Policy({
        case Decision(t@FormatToken(`arrow`, _, _), s) =>
          Decision(t, s.filter(_.modification.isNewline))
      }, expire = lastToken.end))
      .withIndent(2, lastToken, Left) // case body indented by 2.
      .withIndent(2, arrow, Left) // cond body indented by 4.
)
<<< breaking on ( is cheaper than [
val ret =
  new mutable.MapBuilder[TokenHash, Tree, Map[TokenHash, Tree]](Map[TokenHash, Tree]())
>>>
val ret = new mutable.MapBuilder[TokenHash, Tree, Map[TokenHash, Tree]](
    Map[TokenHash, Tree]())
<<< . is cheaper
{{{{
        Seq(
            Split(
                  // This split needs to have an optimalAt field.
                  Space,
                  0,
                  ignoreIf = !spaceCouldBeOk,
                  optimalAt = Some(expire)).withPolicy(SingleLineBlockAAAA(
                expire))
        )

}}}}
>>>
{
  {
    {
      {
        Seq(
            Split(
                  // This split needs to have an optimalAt field.
                  Space,
                  0,
                  ignoreIf = !spaceCouldBeOk,
                  optimalAt = Some(expire))
              .withPolicy(SingleLineBlockAAAA(expire))
        )
      }
    }
  }
}
<<< penalize newlines inside apply 1
val ret =
       new mutable.MapBuilder[TokenHash, Token, Map[TokenHash, Token]](Map
             .empty[TokenHash, Token])
>>>
val ret = new mutable.MapBuilder[TokenHash, Token, Map[TokenHash, Token]](
    Map.empty[TokenHash, Token])
<<< penalize newlines inside apply 2
{{
val result = new scala.collection.mutable.SetBuilder[Token, Set[Token]](Set.empty[Token])
}}
>>>
{
  {
    val result = new scala.collection.mutable.SetBuilder[Token, Set[Token]](
        Set.empty[Token])
  }
}
<<< property scalding
  writeRead(
    // Use list because Array has a shitty toString
    { (b: List[Byte], os) => os.writePosVarInt(b.size); os.writeBytes(b.toArray) },
    { is =>
      val bytes = new Array[Byte](is.readPosVarInt)
      is.readFully(bytes)
      bytes.toList
    })
>>>
writeRead(
          // Use list because Array has a shitty toString
          { (b: List[Byte], os) =>
            os.writePosVarInt(b.size); os.writeBytes(b.toArray)
          }, { is =>
            val bytes = new Array[Byte](is.readPosVarInt)
            is.readFully(bytes)
            bytes.toList
          })
<<< spark configsWithAlternatives
{
val configsWithAlternatives = Map[String, Seq[AlternateConfig]](
    "spark.executor.userClassPathFirst" -> Seq(
      AlternateConfig("spark.files.userClassPathFirst", "1.3")),
    "spark.history.fs.cleaner.interval" -> Seq(
      AlternateConfig("spark.history.fs.cleaner.interval.seconds", "1.4")),
    "spark.history.fs.cleaner.maxAge" -> Seq(
      AlternateConfig("spark.history.fs.cleaner.maxAge.seconds", "1.4")),
    "spark.yarn.am.waitTime" -> Seq(
      AlternateConfig("spark.yarn.applicationMaster.waitTries", "1.3",
        // Translate old value to a duration, with 10s wait time per try.
        translation = s => s"${s.toLong * 10}s")),
    "spark.reducer.maxSizeInFlight" -> Seq(
      AlternateConfig("spark.reducer.maxMbInFlight", "1.4")),
    "spark.kryoserializer.buffer" ->
        Seq(AlternateConfig("spark.kryoserializer.buffer.mb", "1.4",
          translation = s => s"${(s.toDouble * 1000).toInt}k")),
    "spark.yarn.jars" -> Seq(
      AlternateConfig("spark.yarn.jar", "2.0"))
    )
    }
>>>
{
  val configsWithAlternatives =
    Map[String, Seq[AlternateConfig]](
        "spark.executor.userClassPathFirst" -> Seq(
            AlternateConfig("spark.files.userClassPathFirst", "1.3")),
        "spark.history.fs.cleaner.interval" -> Seq(
            AlternateConfig("spark.history.fs.cleaner.interval.seconds",
                            "1.4")),
        "spark.history.fs.cleaner.maxAge" -> Seq(AlternateConfig(
                "spark.history.fs.cleaner.maxAge.seconds", "1.4")),
        "spark.yarn.am.waitTime" -> Seq(AlternateConfig(
                "spark.yarn.applicationMaster.waitTries",
                "1.3",
                // Translate old value to a duration, with 10s wait time per try.
                translation = s => s"${s.toLong * 10}s")),
        "spark.reducer.maxSizeInFlight" -> Seq(
            AlternateConfig("spark.reducer.maxMbInFlight", "1.4")),
        "spark.kryoserializer.buffer" -> Seq(AlternateConfig(
                "spark.kryoserializer.buffer.mb",
                "1.4",
                translation = s => s"${(s.toDouble * 1000).toInt}k")),
        "spark.yarn.jars" -> Seq(AlternateConfig("spark.yarn.jar", "2.0"))
    )
}
<<< SKIP spark comment explodes matrix
{
  test("add") {
    val blocks: Seq[((Int, Int), Matrix)] = Seq(
      ((0, 0), new DenseMatrix(2, 2, Array(1.0, 0.0, 0.0, 2.0))),
      ((0, 1), new DenseMatrix(2, 2, Array(0.0, 1.0, 0.0, 0.0))),
      ((1, 0), new DenseMatrix(2, 2, Array(3.0, 0.0, 1.0, 1.0))),
      ((1, 1), new DenseMatrix(2, 2, Array(1.0, 2.0, 0.0, 1.0))),
      ((2, 0), new DenseMatrix(1, 2, Array(1.0, 0.0))), // This comment will make scalafmt go crazy
      ((2, 1), new DenseMatrix(1, 2, Array(1.0, 5.0))))
      }
}
>>>
x
<<< spark config style dequeue
{{
val options = List[OptionAssigner]( // All cluster managers
      OptionAssigner(args.master, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.master"),
      OptionAssigner(args.deployMode, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,
        sysProp = "spark.submit.deployMode"),
      OptionAssigner(args.name, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.app.name"),
      OptionAssigner(args.jars, ALL_CLUSTER_MGRS, CLIENT, sysProp = "spark.jars"),
      OptionAssigner(args.ivyRepoPath, ALL_CLUSTER_MGRS, CLIENT, sysProp = "spark.jars.ivy"),
      OptionAssigner(args.driverMemory, ALL_CLUSTER_MGRS, CLIENT,
        sysProp = "spark.driver.memory"),
      OptionAssigner(args.driverExtraClassPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,
        sysProp = "spark.driver.extraClassPath"),
      OptionAssigner(args.driverExtraJavaOptions, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,
        sysProp = "spark.driver.extraJavaOptions"),
      OptionAssigner(args.driverExtraLibraryPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,
        sysProp = "spark.driver.extraLibraryPath"),

      // Yarn client only
      OptionAssigner(args.queue, YARN, CLIENT, sysProp = "spark.yarn.queue"),
      OptionAssigner(args.numExecutors, YARN, ALL_DEPLOY_MODES,
        sysProp = "spark.executor.instances"),
      OptionAssigner(args.files, YARN, CLIENT, sysProp = "spark.yarn.dist.files"),
      OptionAssigner(args.archives, YARN, CLIENT, sysProp = "spark.yarn.dist.archives"),
      OptionAssigner(args.principal, YARN, CLIENT, sysProp = "spark.yarn.principal"),
      OptionAssigner(args.keytab, YARN, CLIENT, sysProp = "spark.yarn.keytab"),

      // Yarn cluster only
      OptionAssigner(args.name, YARN, CLUSTER, clOption = "--name"),
      OptionAssigner(args.driverMemory, YARN, CLUSTER, clOption = "--driver-memory"),
      OptionAssigner(args.driverCores, YARN, CLUSTER, clOption = "--driver-cores"),
      OptionAssigner(args.queue, YARN, CLUSTER, clOption = "--queue"),
      OptionAssigner(args.executorMemory, YARN, CLUSTER, clOption = "--executor-memory"),
      OptionAssigner(args.executorCores, YARN, CLUSTER, clOption = "--executor-cores"),
      OptionAssigner(args.files, YARN, CLUSTER, clOption = "--files"),
      OptionAssigner(args.archives, YARN, CLUSTER, clOption = "--archives"),
      OptionAssigner(args.jars, YARN, CLUSTER, clOption = "--addJars"),
      OptionAssigner(args.principal, YARN, CLUSTER, clOption = "--principal"),
      OptionAssigner(args.keytab, YARN, CLUSTER, clOption = "--keytab"),

      // Other options
      OptionAssigner(args.executorCores, STANDALONE | YARN, ALL_DEPLOY_MODES,
        sysProp = "spark.executor.cores"),
      OptionAssigner(args.executorMemory, STANDALONE | MESOS | YARN, ALL_DEPLOY_MODES,
        sysProp = "spark.executor.memory"),
      OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS, ALL_DEPLOY_MODES,
        sysProp = "spark.cores.max"),
      OptionAssigner(args.files, LOCAL | STANDALONE | MESOS, ALL_DEPLOY_MODES,
        sysProp = "spark.files"),
      OptionAssigner(args.jars, STANDALONE | MESOS, CLUSTER, sysProp = "spark.jars"),
      OptionAssigner(args.driverMemory, STANDALONE | MESOS, CLUSTER,
        sysProp = "spark.driver.memory"),
      OptionAssigner(args.driverCores, STANDALONE | MESOS, CLUSTER,
        sysProp = "spark.driver.cores"),
      OptionAssigner(args.supervise.toString, STANDALONE | MESOS, CLUSTER,
        sysProp = "spark.driver.supervise"),
      OptionAssigner(args.ivyRepoPath, STANDALONE, CLUSTER, sysProp = "spark.jars.ivy")
    )
}}
>>>
{
  {
    val options = List[OptionAssigner](
        // All cluster managers
        OptionAssigner(args.master,
                       ALL_CLUSTER_MGRS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.master"),
        OptionAssigner(args.deployMode,
                       ALL_CLUSTER_MGRS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.submit.deployMode"),
        OptionAssigner(args.name,
                       ALL_CLUSTER_MGRS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.app.name"),
        OptionAssigner(
            args.jars, ALL_CLUSTER_MGRS, CLIENT, sysProp = "spark.jars"),
        OptionAssigner(args.ivyRepoPath,
                       ALL_CLUSTER_MGRS,
                       CLIENT,
                       sysProp = "spark.jars.ivy"),
        OptionAssigner(args.driverMemory,
                       ALL_CLUSTER_MGRS,
                       CLIENT,
                       sysProp = "spark.driver.memory"),
        OptionAssigner(args.driverExtraClassPath,
                       ALL_CLUSTER_MGRS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.driver.extraClassPath"),
        OptionAssigner(args.driverExtraJavaOptions,
                       ALL_CLUSTER_MGRS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.driver.extraJavaOptions"),
        OptionAssigner(args.driverExtraLibraryPath,
                       ALL_CLUSTER_MGRS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.driver.extraLibraryPath"),
        // Yarn client only
        OptionAssigner(args.queue, YARN, CLIENT, sysProp = "spark.yarn.queue"),
        OptionAssigner(args.numExecutors,
                       YARN,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.executor.instances"),
        OptionAssigner(
            args.files, YARN, CLIENT, sysProp = "spark.yarn.dist.files"),
        OptionAssigner(
            args.archives, YARN, CLIENT, sysProp = "spark.yarn.dist.archives"),
        OptionAssigner(
            args.principal, YARN, CLIENT, sysProp = "spark.yarn.principal"),
        OptionAssigner(
            args.keytab, YARN, CLIENT, sysProp = "spark.yarn.keytab"),
        // Yarn cluster only
        OptionAssigner(args.name, YARN, CLUSTER, clOption = "--name"),
        OptionAssigner(
            args.driverMemory, YARN, CLUSTER, clOption = "--driver-memory"),
        OptionAssigner(
            args.driverCores, YARN, CLUSTER, clOption = "--driver-cores"),
        OptionAssigner(args.queue, YARN, CLUSTER, clOption = "--queue"),
        OptionAssigner(args.executorMemory,
                       YARN,
                       CLUSTER,
                       clOption = "--executor-memory"),
        OptionAssigner(
            args.executorCores, YARN, CLUSTER, clOption = "--executor-cores"),
        OptionAssigner(args.files, YARN, CLUSTER, clOption = "--files"),
        OptionAssigner(args.archives, YARN, CLUSTER, clOption = "--archives"),
        OptionAssigner(args.jars, YARN, CLUSTER, clOption = "--addJars"),
        OptionAssigner(
            args.principal, YARN, CLUSTER, clOption = "--principal"),
        OptionAssigner(args.keytab, YARN, CLUSTER, clOption = "--keytab"),
        // Other options
        OptionAssigner(args.executorCores,
                       STANDALONE | YARN,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.executor.cores"),
        OptionAssigner(args.executorMemory,
                       STANDALONE | MESOS | YARN,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.executor.memory"),
        OptionAssigner(args.totalExecutorCores,
                       STANDALONE | MESOS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.cores.max"),
        OptionAssigner(args.files,
                       LOCAL | STANDALONE | MESOS,
                       ALL_DEPLOY_MODES,
                       sysProp = "spark.files"),
        OptionAssigner(
            args.jars, STANDALONE | MESOS, CLUSTER, sysProp = "spark.jars"),
        OptionAssigner(args.driverMemory,
                       STANDALONE | MESOS,
                       CLUSTER,
                       sysProp = "spark.driver.memory"),
        OptionAssigner(args.driverCores,
                       STANDALONE | MESOS,
                       CLUSTER,
                       sysProp = "spark.driver.cores"),
        OptionAssigner(args.supervise.toString,
                       STANDALONE | MESOS,
                       CLUSTER,
                       sysProp = "spark.driver.supervise"),
        OptionAssigner(
            args.ivyRepoPath, STANDALONE, CLUSTER, sysProp = "spark.jars.ivy")
    )
  }
}
